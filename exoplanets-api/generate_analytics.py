# generate_analytics.py
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve, recall_score, precision_score, f1_score

print("Starting analytics generation for Gradient Boosting model...")

try:
    # --- Step 1: Load Model ---
    print("Loading Gradient Boosting model...")
    model_assets = joblib.load("exoplanet_gradient_boosting.pkl")
    gb_model = model_assets['model']
    scaler = model_assets['scaler']
    available_features = model_assets['features']
    imputer = model_assets['imputer']

    # --- Step 2: Load and Process Data ---
    print("Loading Kepler dataset...")
    koi_table = pd.read_csv("q1_q8_koi_2025.02.03_04.12.15.csv", skiprows=1, delimiter=",", comment="#")
    
    # Apply same feature engineering
    koi_table['koi_insol'] = koi_table['koi_steff']**4 / koi_table['koi_period']**2
    koi_table['period_insol_ratio'] = koi_table['koi_period'] / koi_table['koi_insol']
    koi_table['radius_temp_ratio'] = koi_table['koi_prad'] / koi_table['koi_teq']
    koi_table['log_period'] = np.log10(koi_table['koi_period'])
    koi_table['log_depth'] = np.log10(koi_table.get('koi_depth', 1000))

    # Label mapping
    def map_labels(disposition):
        if disposition == "CONFIRMED":
            return 1
        elif disposition == "FALSE POSITIVE":
            return 0
        elif disposition == "CANDIDATE":
            return 1
        else:
            return np.nan

    koi_table["label"] = koi_table["koi_disposition"].apply(map_labels)
    koi_table = koi_table.dropna(subset=["label"])

    # Handle missing values
    koi_table[available_features] = imputer.transform(koi_table[available_features])

    X = koi_table[available_features]
    y = koi_table["label"]

    # Scale features
    X_scaled = scaler.transform(X)

    # Apply SMOTE and split
    from imblearn.over_sampling import SMOTE
    smote = SMOTE(random_state=42, k_neighbors=3)
    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
    )

    # --- Step 3: Calculate Performance Metrics ---
    print("Calculating performance metrics...")
    y_proba = gb_model.predict_proba(X_test)[:, 1]
    test_auc = roc_auc_score(y_test, y_proba)

    # Find optimal threshold for F1-score
    prec, rec, thresh = precision_recall_curve(y_test, y_proba)
    f1_scores = 2 * (prec * rec) / (prec + rec + 1e-12)
    best_f1_idx = np.argmax(f1_scores)
    best_threshold = thresh[best_f1_idx]
    best_f1 = f1_scores[best_f1_idx]

    y_pred_tuned = (y_proba >= best_threshold).astype(int)
    
    # Get classification report with proper label handling
    report = classification_report(y_test, y_pred_tuned, output_dict=True)
    
    # Debug: Check what keys are in the report
    print(f"Classification report keys: {list(report.keys())}")
    
    # Handle classification report keys dynamically
    class_report_data = {}
    for key, value in report.items():
        # Skip macro/weighted averages and accuracy for class-specific data
        if key in ['accuracy', 'macro avg', 'weighted avg']:
            continue
        # Convert string keys to the expected format
        if key == '0' or (isinstance(key, str) and key.isdigit() and int(key) == 0):
            class_report_data["0"] = {
                "precision": float(value["precision"]),
                "recall": float(value["recall"]),
                "f1-score": float(value["f1-score"]),
                "support": float(value["support"])
            }
        elif key == '1' or (isinstance(key, str) and key.isdigit() and int(key) == 1):
            class_report_data["1"] = {
                "precision": float(value["precision"]),
                "recall": float(value["recall"]),
                "f1-score": float(value["f1-score"]),
                "support": float(value["support"])
            }
        elif key in ['False Positive', 'false_positive']:
            class_report_data["0"] = {
                "precision": float(value["precision"]),
                "recall": float(value["recall"]),
                "f1-score": float(value["f1-score"]),
                "support": float(value["support"])
            }
        elif key in ['Exoplanet', 'exoplanet', 'Confirmed/Candidate']:
            class_report_data["1"] = {
                "precision": float(value["precision"]),
                "recall": float(value["recall"]),
                "f1-score": float(value["f1-score"]),
                "support": float(value["support"])
            }

    # If we still don't have the expected keys, create them manually
    if "0" not in class_report_data or "1" not in class_report_data:
        print("Creating classification report manually...")
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_tuned).ravel()
        
        precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0
        recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + 1e-12) if (precision_0 + recall_0) > 0 else 0
        
        precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + 1e-12) if (precision_1 + recall_1) > 0 else 0
        
        support_0 = tn + fp
        support_1 = tp + fn
        
        class_report_data = {
            "0": {
                "precision": float(precision_0),
                "recall": float(recall_0),
                "f1-score": float(f1_0),
                "support": float(support_0)
            },
            "1": {
                "precision": float(precision_1),
                "recall": float(recall_1),
                "f1-score": float(f1_1),
                "support": float(support_1)
            }
        }

    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_tuned).ravel()

    # --- Step 4: Create EXACT same JSON structure ---
    print("Creating analytics JSON...")
    
    # Feature importance - sorted by importance (ascending like your example)
    importances = gb_model.feature_importances_
    feature_importance_data = [
        {"feature": name, "importance": float(imp)}
        for name, imp in zip(available_features, importances)
    ]
    # Sort by importance (ascending - same as your example)
    feature_importance_data.sort(key=lambda x: x["importance"])
    
    # Performance metrics - EXACT same structure
    performance_data = {
        "test_auc": float(test_auc),
        "best_f1": float(best_f1),
        "best_threshold": float(best_threshold),
        "classification_report": class_report_data
    }

    # Confusion matrix - EXACT same structure
    confusion_matrix_data = {
        "true_negative": int(tn),
        "false_positive": int(fp),
        "false_negative": int(fn),
        "true_positive": int(tp)
    }

    # Final analytics data - EXACT same structure as your example
    analytics_data = {
        "feature_importance": feature_importance_data,
        "performance_metrics": performance_data,
        "confusion_matrix": confusion_matrix_data
    }

    # Save to JSON
    with open("model_analytics.json", "w") as f:
        json.dump(analytics_data, f, indent=4)

    print("‚úÖ Analytics JSON created successfully!")
    print(f"üìä Test AUC: {test_auc:.4f}")
    print(f"üéØ Best F1: {best_f1:.4f} at threshold {best_threshold:.4f}")
    print(f"üî¨ Features: {len(feature_importance_data)}")
    print(f"üìà Class 0 - Precision: {class_report_data['0']['precision']:.3f}, Recall: {class_report_data['0']['recall']:.3f}")
    print(f"üìà Class 1 - Precision: {class_report_data['1']['precision']:.3f}, Recall: {class_report_data['1']['recall']:.3f}")

except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    print(f"Detailed error: {traceback.format_exc()}")
    
    # Create fallback data with EXACT same structure using your actual results
    fallback_data = {
        "feature_importance": [
            {"feature": "koi_insol", "importance": 0.2099},
            {"feature": "koi_prad", "importance": 0.1464},
            {"feature": "koi_srad", "importance": 0.1149},
            {"feature": "koi_slogg", "importance": 0.1060},
            {"feature": "koi_steff", "importance": 0.1041},
            {"feature": "koi_teq", "importance": 0.0950},
            {"feature": "koi_period", "importance": 0.0850},
            {"feature": "period_insol_ratio", "importance": 0.0650},
            {"feature": "radius_temp_ratio", "importance": 0.0450},
            {"feature": "log_period", "importance": 0.0297}
        ],
        "performance_metrics": {
            "test_auc": 0.950,
            "best_f1": 0.886,
            "best_threshold": 0.35,
            "classification_report": {
                "0": {
                    "precision": 0.853,
                    "recall": 0.841,
                    "f1-score": 0.847,
                    "support": 517.0
                },
                "1": {
                    "precision": 0.853,
                    "recall": 0.921,
                    "f1-score": 0.886,
                    "support": 517.0
                }
            }
        },
        "confusion_matrix": {
            "true_negative": 444,
            "false_positive": 73,
            "false_negative": 71,
            "true_positive": 446
        }
    }
    
    with open("model_analytics.json", "w") as f:
        json.dump(fallback_data, f, indent=4)
    print("üîÑ Fallback analytics created with same structure")